{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738e4bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests #common libraries to install for any type of scraping\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint,choice\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\",[\"enable-automation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa6c75",
   "metadata": {},
   "source": [
    "# Fintech Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://courses.cfte.education/ranking-of-largest-fintech-companies/',verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c92b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e24ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = soup.find_all(\"td\")\n",
    "i = 0\n",
    "for i in range(1,len(a),i+11):\n",
    "    if str(a[i].string) != 'None' and str(a[i+1].string) == 'United States':\n",
    "        print(str(a[i].string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930b6b6",
   "metadata": {},
   "source": [
    "# Clutch Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36'},\n",
    "{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 Edg/105.0.1343.33'},\n",
    "{'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:104.0) Gecko/20100101 Firefox/104.0'},\n",
    "{'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 OPR/90.0.4480.84'}]\n",
    "\n",
    "data = {\n",
    "  \"name\": [],\n",
    "  \"description\": [],\n",
    "  'Employees': [],\n",
    "  'Founded': [],\n",
    "  \"all_locations\": [],\n",
    "  \"Service lines\": [],\n",
    "  \"Client focus\":[],\n",
    "  \"Industry focus\":[],\n",
    "  \"Clients Name\": [],\n",
    "  \"Website\": [],\n",
    "  \"linkedin\": []\n",
    "}\n",
    "graph = ['Service lines', 'Client focus', 'Industry focus']\n",
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='search links')\n",
    "start_page = int(dataframe['start page'][len(dataframe)-1])\n",
    "last_page = int(dataframe['last page'][len(dataframe)-1])\n",
    "for i in range(start_page, last_page+1):\n",
    "    try:\n",
    "        r = requests.get(dataframe['link'][len(dataframe)-1]+str(i),verify=False,headers = headers[randint(0,3)]) #put link here\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content, 'html.parser')\n",
    "            company_name = soup.find_all(class_=\"company_info\")\n",
    "            website_profile = soup.find_all(class_=\"website-profile\")\n",
    "            print('succesfully loaded Page'+str(i+1))\n",
    "            for j in range(len(company_name)):\n",
    "                try:\n",
    "                    r1 = requests.get('https://clutch.co'+website_profile[j].a['href'], verify=False, headers = headers[randint(0,3)])\n",
    "                    soup1 = BeautifulSoup(r1.content, 'html.parser')\n",
    "                    data['name'].append(\" \".join(company_name[j].a.string.split()))\n",
    "                    if r1.status_code == 200:\n",
    "                        graph_name = []\n",
    "                        focus = soup1.find_all(class_='chartAreaContainer spm-bar-chart')\n",
    "                        if soup1.find(class_='field-name-clients') != None:\n",
    "                            data['Clients Name'].append(soup1.find(class_='field-name-clients').find('div',class_='field-item').text.strip())\n",
    "                        else:\n",
    "                            data['Clients Name'].append(' ')\n",
    "                        for d in soup1.find_all(class_='graph-title'):\n",
    "                            graph_name.append(d.text)\n",
    "                        for p in graph:\n",
    "                            if p in graph_name:\n",
    "                                indx = graph_name.index(p)\n",
    "                                focus_details = ''\n",
    "                                for c in focus[indx].find_all('div'):\n",
    "                                    focus_details += c.text + ' ' + c['data-content'][3:-4] + ', ' \n",
    "                                data[p].append(focus_details)\n",
    "                            else:\n",
    "                                data[p].append(' ')\n",
    "                        locations = ''\n",
    "                        for m in range(len(soup1.find_all(class_ = 'locality'))):\n",
    "                            locations += soup1.find_all(class_='locality')[m].text + ' ' +soup1.find_all(class_='country-name')[m].text+'; '\n",
    "                        data['all_locations'].append(locations)\n",
    "                        if soup1.find(class_='field-name-profile-summary') != None:\n",
    "                            data['description'].append(soup1.find(class_='field-name-profile-summary').text.strip())\n",
    "                        else:\n",
    "                            data['description'].append('')\n",
    "                        if soup1.find('div', attrs={'data-content':'<i>Employees</i>'}) != None:\n",
    "                            data['Employees'].append(soup1.find('div', attrs={'data-content':'<i>Employees</i>'}).text.strip())\n",
    "                            data['Founded'].append(soup1.find('div', attrs={'data-content':'<i>Founded</i>'}).text.strip())\n",
    "                        else:\n",
    "                            data['Employees'].append('')\n",
    "                            data['Founded'].append('')\n",
    "                        if soup1.find(class_='website-link-a') != None:\n",
    "                            data['Website'].append(soup1.find(class_='website-link-a').a['href'])\n",
    "                        else:\n",
    "                            data['Website'].append('')\n",
    "                        if soup1.find(class_='linkedin') != None:\n",
    "                            url = soup1.find(class_='linkedin')['href']\n",
    "                            data['linkedin'].append(url)\n",
    "                        else:\n",
    "                            data['linkedin'].append('')\n",
    "                        print('successfully scraped company '+str(j+1))\n",
    "                    else:\n",
    "                        print('Bad Response from website for company '+str(j+1)+' details')\n",
    "                        for key in data.keys():\n",
    "                            if key != 'name':\n",
    "                                data[key].append('')\n",
    "                    df = pd.DataFrame(data)\n",
    "                    with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "                        df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "                        df.to_excel(writer, sheet_name=\"result\")\n",
    "                    sleep(randint(3,10))\n",
    "                except:\n",
    "                    print('error loading company '+str(j+1)+' details')\n",
    "                    for key in data.keys():\n",
    "                        if key != 'name':\n",
    "                            data[key].append('')\n",
    "        else:\n",
    "            print('Bad Response from website for page '+str(i+1)+ ' Skipped to next page.')\n",
    "    except:\n",
    "        print('error loading page '+str(i+1)+' Skipped to next page.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ef376",
   "metadata": {},
   "source": [
    "# LinkedIn Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93540576",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=Service(r'') #login code\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "driver.get(\"https://www.linkedin.com/checkpoint/lg/sign-in-another-account\")\n",
    "WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit']\")))\n",
    "username = driver.find_element(By.ID,\"username\")\n",
    "username.send_keys(\"\")\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "pword.send_keys(\"\")\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa91f8",
   "metadata": {},
   "source": [
    "### Linkedin scraping 1: when company links are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b83339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_link(i, word):\n",
    "    if word in i:\n",
    "        indx = i.index(word)\n",
    "        return i[:indx]\n",
    "    else:\n",
    "        return i\n",
    "profile = {'name': [],\n",
    "           'Description': [],\n",
    "           'Founded': [],\n",
    "           'Industry': [],\n",
    "           'Company size': [],\n",
    "           'Headquarters': [],\n",
    "           'Total Employees': [],\n",
    "           'Employees by Geo': [],\n",
    "           'Specialties':[],\n",
    "           'Website': []\n",
    "          }\n",
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='linkedin links')\n",
    "dataframe['linkedin'] = dataframe['linkedin'].fillna('')\n",
    "info_heading = ['Industry', 'Company size', 'Headquarters', 'Founded', 'Specialties', 'Website' ]\n",
    "words = ['about','mycompany','?view','?trk','admin']\n",
    "j=0\n",
    "for i in dataframe['linkedin']:\n",
    "    try:\n",
    "        if len(i) != 0 and '/company/' in i:\n",
    "            for word in words:\n",
    "                i = check_link(i,word)\n",
    "            if i[-1] != '/':\n",
    "                i = i + '/'\n",
    "            driver.get(i +'about/')\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"org-grid__content-height-enforcer\")))\n",
    "            soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            profile['name'].append(dataframe['name'][j])\n",
    "            if soup1.find('p', class_='break-words white-space-pre-wrap mb5 text-body-small t-black--light') != None:\n",
    "                profile['Description'].append(soup1.find('p', class_='break-words white-space-pre-wrap mb5 text-body-small t-black--light').text.strip())\n",
    "            else:\n",
    "                profile['Description'].append(' ')\n",
    "            real_info_heading = [k.text.strip() for k in soup1.find_all('dt', class_='text-heading-small')]\n",
    "            real_info_tag = [m for m in soup1.find_all('dt', class_='text-heading-small')]\n",
    "            for p in info_heading:\n",
    "                if p in real_info_heading:\n",
    "                    indx = real_info_heading.index(p)\n",
    "                    profile[p].append(real_info_tag[indx].find_next('dd').text.strip()) \n",
    "                else:\n",
    "                    profile[p].append(' ')\n",
    "            try:\n",
    "                driver.get(i +'people/')\n",
    "                WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, \"artdeco-carousel__item-container\")))\n",
    "                soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                if soup1.find('h2', class_='t-20 t-black t-bold') != None:\n",
    "                    profile['Total Employees'].append(soup1.find('h2', class_='t-20 t-black t-bold').text.strip())\n",
    "                else:\n",
    "                    profile['Total Employees'].append('')\n",
    "                if soup1.find('div',class_='artdeco-carousel__item-container') != None:\n",
    "                    container =  soup1.find('div',class_='artdeco-carousel__item-container')\n",
    "                    container_items = container.find_all('div',class_='org-people-bar-graph-element__percentage-bar-info truncate full-width mt2 mb1 t-14 t-black--light t-normal')\n",
    "                    people = ''\n",
    "                    for s in range(len(container_items)):\n",
    "                        if s<4:\n",
    "                            people += container_items[s].text.strip() + '; '\n",
    "                    profile['Employees by Geo'].append(people)\n",
    "                else:\n",
    "                    profile['Employees by Geo'].append('')\n",
    "                sleep(randint(3,10))\n",
    "            except Exception as e:\n",
    "                print(\"error in people section of company\"+str(j+1)+\" \"+str(e))\n",
    "                profile['Total Employees'].append('')\n",
    "                profile['Employees by Geo'].append('')\n",
    "            df = pd.DataFrame(profile)\n",
    "            with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "                df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "                df.to_excel(writer, sheet_name=\"result\")\n",
    "            print('company '+str(j+1)+\" scraped and stored\")\n",
    "    except Exception as e:\n",
    "        print(\"error on company\"+str(j+1)+str(e))\n",
    "    j += 1\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a0758",
   "metadata": {},
   "source": [
    "### Linkedin scraping 2: when a single search link is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b95544",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile = {'name': [],\n",
    "           'Description': [],\n",
    "           'Founded': [],\n",
    "           'Industry': [],\n",
    "           'Company size': [],\n",
    "           'Headquarters': [],\n",
    "           'Total Employees': [],\n",
    "           'Where they live': [],\n",
    "           'Where they studied':[],\n",
    "           'What they do':[],\n",
    "           'What they are skilled at':[],\n",
    "           'What they studied':[],\n",
    "           'Specialties':[],\n",
    "           'Website': [],\n",
    "           'linkedin link':[]\n",
    "          }\n",
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='search links')\n",
    "start_page = int(dataframe['start page'][len(dataframe)-1])\n",
    "last_page = int(dataframe['last page'][len(dataframe)-1])\n",
    "info_heading = ['Industry', 'Company size', 'Headquarters', 'Founded', 'Specialties', 'Website' ]\n",
    "info_people = ['Where they live','Where they studied','What they do','What they are skilled at','What they studied']\n",
    "\n",
    "for page in range(start_page,last_page+1):\n",
    "    try:\n",
    "        driver.get(dataframe['link'][len(dataframe)-1]+str(page))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"entity-result__item\")))\n",
    "        sleep(1)\n",
    "        print('page '+str(page))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        companies_single_page = soup.find_all(class_='entity-result__item')\n",
    "        for i in range(len(companies_single_page)):\n",
    "            try:\n",
    "                driver.get(companies_single_page[i].find(class_='app-aware-link')['href']+'about/')\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"org-grid__content-height-enforcer\")))\n",
    "                soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                nav_bar = [i.text.strip() for i in soup1.find_all(class_='org-page-navigation__item-anchor')]\n",
    "                profile['name'].append(companies_single_page[i].find_all(class_='app-aware-link')[1].text.strip())\n",
    "                profile['linkedin link'].append(companies_single_page[i].find(class_='app-aware-link')['href'])\n",
    "                if soup1.find('p', class_='break-words white-space-pre-wrap t-black--light mb5 text-body-small') != None:\n",
    "                    profile['Description'].append(soup1.find('p', class_='break-words white-space-pre-wrap t-black--light mb5 text-body-small').text.strip())\n",
    "                else:\n",
    "                    profile['Description'].append(' ')\n",
    "                real_info_heading = [k.text.strip() for k in soup1.find_all('dt', class_='text-heading-small')]\n",
    "                real_info_tag = [m for m in soup1.find_all('dt', class_='text-heading-small')]\n",
    "                for p in info_heading:\n",
    "                    if p in real_info_heading:\n",
    "                        indx = real_info_heading.index(p)\n",
    "                        profile[p].append(real_info_tag[indx].find_next('dd').text.strip()) \n",
    "                    else:\n",
    "                        profile[p].append(' ')\n",
    "                if 'People' in nav_bar:\n",
    "                    try:        \n",
    "                        driver.get(companies_single_page[i].find(class_='app-aware-link')['href'] +'people/')\n",
    "                        WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, \"artdeco-carousel__item-container\")))\n",
    "                        q = 0\n",
    "                        soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                        profile['Total Employees'].append(soup1.find('h2', class_='t-20 t-black t-bold').text.strip())\n",
    "                        for y in range(3):\n",
    "                            for z in range(0,2):\n",
    "                                if q < 5:\n",
    "                                    soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                                    container = soup1.find_all('div',class_='artdeco-carousel__item-container')\n",
    "                                    container_items = container[q].find_all('div',class_='org-people-bar-graph-element__percentage-bar-info truncate full-width mt2 mb1 t-14 t-black--light t-normal')\n",
    "                                    people = ''\n",
    "                                    for s in range(len(container_items)):\n",
    "                                        people += container_items[s].text.strip() + '; '\n",
    "                                    profile[info_people[q]].append(people)\n",
    "                                    q += 1\n",
    "                            driver.execute_script(\"arguments[0].click();\", driver.find_element(By.XPATH, \"//button[@aria-label='Next']\"))\n",
    "                    except Exception as e:\n",
    "                        for heading in info_people:\n",
    "                            profile[heading].append('')\n",
    "                        profile['Total Employees'].append('')\n",
    "                        if soup1.find('h2', class_='t-20 t-black t-bold') == None:\n",
    "                            driver.get_screenshot_as_file(\"screenshot.png\")\n",
    "                        print('company'+str(i+1)+' got error: '+str(e))\n",
    "                else:\n",
    "                    for heading in info_people:\n",
    "                        profile[heading].append('')\n",
    "                    profile['Total Employees'].append('')\n",
    "                df = pd.DataFrame(profile)\n",
    "                with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "                    df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "                    df.to_excel(writer, sheet_name=\"result\")\n",
    "                print('company '+str(i+1)+' saved')\n",
    "                sleep(randint(3,10))\n",
    "            except Exception as e:\n",
    "                print(\"error in second loop of company \"+str(i+1)+\" \"+str(e))\n",
    "    except Exception as e:\n",
    "        print(\"error in first loop on page \"+str(page)+str(e))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82982605",
   "metadata": {},
   "source": [
    "### Linkedin scraping 3: when connection link is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = {'Name': [],\n",
    "           'Description': [],\n",
    "           'Place': [],\n",
    "           'Profile link':[]\n",
    "          }\n",
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='search links')\n",
    "start_page = int(dataframe['start page'][len(dataframe)-1])\n",
    "last_page = int(dataframe['last page'][len(dataframe)-1])\n",
    "for page in range(start_page,last_page+1):\n",
    "    try:\n",
    "        driver.get(dataframe['link'][len(dataframe)-1]+str(page))\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"entity-result__item\")))\n",
    "        sleep(1)\n",
    "        print('page '+str(page))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        single_page_profile = soup.find_all(class_='entity-result__item')\n",
    "        for i in range(len(single_page_profile)):\n",
    "            if single_page_profile[i].find_all(class_='app-aware-link')[1].span != None:\n",
    "                profile['Name'].append(single_page_profile[i].find_all(class_='app-aware-link')[1].span.span.text.strip())\n",
    "            else:\n",
    "                profile['Name'].append(single_page_profile[i].find_all(class_='app-aware-link')[1].text.strip())\n",
    "            profile['Profile link'].append(single_page_profile[i].find(class_='app-aware-link')['href'])\n",
    "            if single_page_profile[i].find(class_='entity-result__primary-subtitle') != None:\n",
    "                profile['Description'].append(single_page_profile[i].find(class_='entity-result__primary-subtitle').text.strip())\n",
    "            else:\n",
    "                profile['Description'].append('')\n",
    "            if single_page_profile[i].find(class_='entity-result__secondary-subtitle') != None:\n",
    "                profile['Place'].append(single_page_profile[i].find(class_='entity-result__secondary-subtitle').text.strip())\n",
    "            else:\n",
    "                profile['Place'].append('')\n",
    "            df = pd.DataFrame(profile)\n",
    "            with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "                df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "                df.to_excel(writer, sheet_name=\"result\")\n",
    "            print('profile '+str(i+1)+' saved')\n",
    "        sleep(randint(3,10))\n",
    "    except Exception as e:\n",
    "        print(\"page \"+str(page)+\" got error: \"+str(e))\n",
    "        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea690d",
   "metadata": {},
   "source": [
    "### Linkedin scraping 4: Request sending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb826f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='linkedin links')\n",
    "profile_links = dataframe['linkedin']\n",
    "status = {\n",
    "    'profile link':[],\n",
    "    'status':[]\n",
    "}\n",
    "for i in profile_links:\n",
    "    try:\n",
    "        status['profile link'].append(i)\n",
    "        driver.get(i)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, \"//div[@class='pv-top-card-v2-ctas ']//li-icon[@type='download']\")))\n",
    "        connect = driver.find_elements(By.XPATH,\"//div[@class='pv-top-card-v2-ctas ']//li-icon[@type='connect']\")\n",
    "        pending = driver.find_elements(By.XPATH,\"//div[@class='pv-top-card-v2-ctas ']//li-icon[@type='clock']\")\n",
    "        if len(connect) != 0:\n",
    "            driver.execute_script(\"arguments[0].click();\", connect[0])\n",
    "            WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.XPATH, \"//button[@aria-label='Send now']\"))).click()\n",
    "            print(\"Connection Request Sent Successfully to: \"+str(i))\n",
    "            status['status'].append(\"Connection Request Sent Successfully\")\n",
    "        elif len(pending) != 0:\n",
    "            print(\"Pending request to: \"+str(i))\n",
    "            status['status'].append(\"Pending request\")\n",
    "        else:\n",
    "            print(\"Already a connection: \"+str(i))\n",
    "            status['status'].append(\"Already a connection\")\n",
    "        sleep(randint(3,10))      \n",
    "    except Exception as e:\n",
    "        print(\"Sending connection Request Failed for: \"+str(i)+\" .Due to: \"+str(e))\n",
    "        status['status'].append(\"Error sending request\")\n",
    "    finally:\n",
    "        df = pd.DataFrame(status)\n",
    "        with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "            df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "            df.to_excel(writer, sheet_name=\"result\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0be62a",
   "metadata": {},
   "source": [
    "# Axial Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=Service(r'')\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='search links')\n",
    "link = dataframe['link'][len(dataframe)-1]\n",
    "start_page = dataframe['start page'][len(dataframe)-1]\n",
    "last_page = dataframe['last page'][len(dataframe)-1]\n",
    "data = {\n",
    "    'name':[],\n",
    "    'website':[],\n",
    "    'overview':[],\n",
    "    'industries':[],\n",
    "    'transactions':[],\n",
    "    'locations':[],\n",
    "    'team':[]\n",
    "}\n",
    "for i in range(start_page,last_page+1):\n",
    "    try:\n",
    "        driver.get(link + str(i) + \"/\")\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        profile_links = soup.find_all(class_='teaser1-title')\n",
    "        print('successfully loaded page '+str(i))\n",
    "        for j in range(len(profile_links)):\n",
    "            data['name'].append(profile_links[j].text.strip())\n",
    "            try:\n",
    "                driver.get(profile_links[j].a['href'])\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"grid__col-9\")))\n",
    "                for k in driver.find_elements(By.XPATH, \"//span[@class='axl-caret--down ng-star-inserted']\"):\n",
    "                    driver.execute_script(\"arguments[0].click();\", k)\n",
    "                for l in driver.find_elements(By.XPATH, \"//i[@class='material-icons industry-accordion__icon']\"):\n",
    "                    driver.execute_script(\"arguments[0].click();\", l)\n",
    "                soup1 = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                if len(soup1.find_all(lambda tag: tag.name == 'a' and tag.get('class') == ['u-link'])) != 0:\n",
    "                    data['website'].append(soup1.find_all(lambda tag: tag.name == 'a' and tag.get('class') == ['u-link'])[0]['href'])\n",
    "                else:\n",
    "                    data['website'].append('')\n",
    "                if soup1.find('account-profile-overview') != None:\n",
    "                    data['overview'].append(soup1.find('account-profile-overview').find(class_='grid').text.replace('\\n',''))\n",
    "                else:\n",
    "                    data['overview'].append('')\n",
    "                industries = ''\n",
    "                industry_container = soup1.find_all('div', class_='accordion__tab ng-star-inserted')\n",
    "                for m in industry_container:\n",
    "                    industries += m.find(class_='u-text-body').text.replace('\\n','') + '\\n'\n",
    "                    for n in m.find_all(class_='u-text-body ng-star-inserted'):\n",
    "                        industries += n.text.strip() + '; '\n",
    "                    industries += '\\n\\n'\n",
    "                data['industries'].append(industries)\n",
    "                if soup1.find(id='transactions') != None:\n",
    "                    data['transactions'].append(\" \".join(soup1.find(id='transactions').find(class_='card__title u-text-heading u-text-heading--xxsmall').text.split()))\n",
    "                else:\n",
    "                    data['transactions'].append('')\n",
    "                location_container = soup1.find_all(class_='account-profile-office grid__col-4 grid__col--top-bottom-gutters ng-star-inserted')\n",
    "                locations = ''\n",
    "                for o in location_container:\n",
    "                    locations += ' '.join(o.text.split()[1:])\n",
    "                    locations += '\\n'\n",
    "                data['locations'].append(locations)\n",
    "                team = ''\n",
    "                for p in soup1.find_all(class_='member grid'):\n",
    "                    team += ' '.join(p.find(class_='grid__col-8').text.split()) + '\\n'\n",
    "                data['team'].append(team)\n",
    "                print('successfully scraped company '+str(j+1))\n",
    "                sleep(randint(3,10))\n",
    "            except:\n",
    "                print('error loading company '+str(j+1)+' details')\n",
    "                for key in data.keys():\n",
    "                    if key != 'name':\n",
    "                        data[key].append('')\n",
    "    except:\n",
    "        print('error loading page '+str(i)+' Skipped to next page.')\n",
    "df = pd.DataFrame(data)\n",
    "with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "    df.to_excel(writer, sheet_name=\"result\") \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb58be0",
   "metadata": {},
   "source": [
    "## Life sciences USA scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ba5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = {'Person': [],\n",
    "           'Position': [],\n",
    "           'Organisation': [],\n",
    "           'Country': [],\n",
    "           'Org Description': [],\n",
    "           'Website': [],\n",
    "           'Linkedin': [],\n",
    "           'Group': [],\n",
    "           'Industry': []\n",
    "          }\n",
    "s=Service(r'')\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "columns = ['Group', 'Industry']\n",
    "extra_details = ['Org Description','Website','Linkedin','Group','Industry']\n",
    "for i in range(84,119):\n",
    "    try:\n",
    "        driver.get(\"https://www.life-sciences-usa.com/person/transatlantic-europe-israel-america-canada-persons-people-addresses-contacts-list-biotech-medtech-1001-1063-43-\"+str(i)+\"-1-asc.html\")\n",
    "        print(\"successfully loaded page \"+str(i+1)+' .Fetching details of each company now......')\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        single_page = soup.find_all('tr')\n",
    "        for j in range(1,len(single_page)):\n",
    "            single_row = single_page[j].find_all('td')\n",
    "            profile['Person'].append(single_row[0].text.strip())\n",
    "            profile['Position'].append(single_row[1].text.strip())\n",
    "            profile['Organisation'].append(single_row[2].text.strip())\n",
    "            profile['Country'].append(single_row[3].text.strip())\n",
    "            try:\n",
    "                driver.get(\"https://www.life-sciences-usa.com\"+single_row[0].a['href'])\n",
    "                sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                col_names = [col.find_all('td')[1].text for col in soup.find_all('tbody')[0].find_all('tr')]\n",
    "                link = \"\"\n",
    "                if col_names[0] == \"Organisation\":\n",
    "                    link = soup.find_all('tbody')[0].find_all('tr')[0].find_all('td')[2].a['href']\n",
    "                if len(link) != 0:\n",
    "                    try:\n",
    "                        driver.get(\"https://www.life-sciences-usa.com\"+link)\n",
    "                        sleep(2)\n",
    "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                        if len(soup.find_all('p',class_ = \"iconlines\")) != 0:\n",
    "                            website=''\n",
    "                            linkedin = ''\n",
    "                            for l in soup.find_all('p',class_ = \"iconlines\"):\n",
    "                                if len(l.find_all('img')) != 0:\n",
    "                                    website = l.text\n",
    "                                    profile['Website'].append(website)\n",
    "                                elif \"LinkedIn\" in l.text:\n",
    "                                    linkedin = l.text[9:].strip()\n",
    "                                    profile['Linkedin'].append(linkedin)\n",
    "                            if len(website) == 0:\n",
    "                                profile['Website'].append('')\n",
    "                            if len(linkedin) == 0:\n",
    "                                profile['Linkedin'].append('')\n",
    "                        else:\n",
    "                            profile['Website'].append('')\n",
    "                            profile['Linkedin'].append('')\n",
    "                        table = soup.find('table')\n",
    "                        if table != None:\n",
    "                            col_names2 = [col.find_all('td')[1].text for col in table.find_all('tr')]\n",
    "                            prev_ele = list(table.previous_siblings)\n",
    "                            prev_ele = [i for i in prev_ele if i != '\\n']\n",
    "                            if prev_ele[0].name == 'p' and prev_ele[1].name == 'p':\n",
    "                                profile['Org Description'].append(prev_ele[1].text.strip())\n",
    "                            else:\n",
    "                                profile['Org Description'].append('')\n",
    "                            for item in columns:\n",
    "                                if item in col_names2:\n",
    "                                    indx = col_names2.index(item)\n",
    "                                    profile[item].append(table.find_all('tr')[indx].find_all('td')[2].text.strip())\n",
    "                                else:\n",
    "                                    profile[item].append('')\n",
    "                            print(\"successfully scraped company \"+str(j))\n",
    "                        else:\n",
    "                            for key in extra_details:\n",
    "                                if key != 'Website' and key != 'Linkedin':\n",
    "                                    profile[key].append('')\n",
    "                            print('check this')\n",
    "                            print(\"successfully scraped company \"+str(j))\n",
    "                    except Exception as e:\n",
    "                        print(\"cant get further company \"+str(j)+\" details because last page not loaded due to \"+str(e))\n",
    "                        for key in extra_details:\n",
    "                            profile[key].append('')\n",
    "                else:\n",
    "                    print(\"cant get further company \"+str(j)+\" details because there is no link on the web page\")\n",
    "                    for key in extra_details:\n",
    "                        profile[key].append('')        \n",
    "            except Exception as e:\n",
    "                print(\"company \"+str(j)+\" not loaded due to \"+str(e))\n",
    "                for key in extra_details:\n",
    "                    profile[key].append('')\n",
    "            df = pd.DataFrame(profile)\n",
    "            with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "                df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "                df.to_excel(writer, sheet_name=\"result\")\n",
    "            print(\"successfully saved fetched details of company \"+str(j))\n",
    "            print()\n",
    "            sleep(randint(3,10))\n",
    "    except Exception as e:\n",
    "        print(\"page \"+str(i+1)+\" not loaded due to \"+str(e))\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5b41f",
   "metadata": {},
   "source": [
    "# Techreviewer scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = {\n",
    "  \"name\": [],\n",
    "  \"description\": [],\n",
    "  'Company size': [],\n",
    "  'Founded': [],\n",
    "  \"Client focus\":[],\n",
    "  'Headquarters':[],\n",
    "  \"Other locations\": [],\n",
    "  \"Service Lines\": [],\n",
    "  \"Domain focus\":[],\n",
    "  \"Frameworks\":[],\n",
    "  \"CMS solutions\":[],\n",
    "  \"Major Clients\": [],\n",
    "  \"Website\": [],\n",
    "  \"linkedin\": []\n",
    "}\n",
    "extra = ['Domain focus', 'CMS solutions', 'Frameworks', 'Service Lines']\n",
    "s=Service(r'')\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "dataframe = pd.read_excel('MnA Details.xlsx', sheet_name='search links')\n",
    "start_page = int(dataframe['start page'][len(dataframe)-1])\n",
    "last_page = int(dataframe['last page'][len(dataframe)-1])\n",
    "for page in range(start_page,last_page+1):\n",
    "    try:\n",
    "        count = 1\n",
    "        if count != 1:\n",
    "            driver.execute_script(\"arguments[0].click();\", driver.find_element(By.XPATH, \"//a[@rel='next']\"))\n",
    "        else:\n",
    "            driver.get(dataframe['link'][len(dataframe)-1]+str(page))\n",
    "        print('page '+str(page))\n",
    "        for i in range(len(driver.find_elements(By.CLASS_NAME, \"company-item__profile-btn\"))):\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", driver.find_elements(By.CLASS_NAME, \"company-item__profile-btn\")[i])\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                profile['name'].append(soup.find(class_=\"company-info-header__name\").text.strip())\n",
    "                profile['Website'].append(soup.find(class_='company-info-header__aside').a['href'])\n",
    "                profile['Company size'].append(soup.find_all(class_='company-info-features__item')[2].text.strip())\n",
    "                profile['Client focus'].append(soup.find_all(class_='company-info-features__item')[3].text.strip())\n",
    "                profile['Headquarters'].append(soup.find_all(class_='company-info-features__item')[4].text.strip())\n",
    "                profile['Founded'].append(soup.find_all(class_='company-info-features__item')[5].text.strip())\n",
    "                if soup.find(class_='company-info__content') != None:\n",
    "                    profile['description'].append(' '.join(soup.find(class_='company-info__content').text.split()))\n",
    "                else:\n",
    "                    profile['description'].append('')\n",
    "                locations = ''\n",
    "                for loc in range(len(soup.find_all(class_='country-and-city'))):\n",
    "                    if loc != 1:\n",
    "                        locations = locations + '\\n' + soup.find_all(class_='country-and-city')[loc].text\n",
    "                profile['Other locations'].append(locations.strip())\n",
    "                linkedin = ''\n",
    "                for link in soup.find_all(class_='company-locations__socials-link'):\n",
    "                    if 'linkedin' in link['href']:\n",
    "                        linkedin = link['href']\n",
    "                        profile['linkedin'].append(linkedin)\n",
    "                if len(linkedin) == 0:\n",
    "                    profile['linkedin'].append(linkedin)\n",
    "                clients= ''\n",
    "                for client in range(len(soup.find_all(class_='major-client-item'))):\n",
    "                    clients = clients + '\\n' + soup.find_all(class_='major-client-item')[client].text\n",
    "                profile['Major Clients'].append(clients.strip())\n",
    "                headers = [item.text.strip() for item in soup.find_all(class_='category-block-header')]\n",
    "                for ele in extra:\n",
    "                    if ele in headers:\n",
    "                        indx = headers.index(ele) \n",
    "                        rows = ''\n",
    "                        for row in soup.find_all(class_='category-block-content')[indx].tbody.find_all('tr'):\n",
    "                            row = row.find(class_='category-name').text.strip() + ' ' + row.find(class_='category-percentage').text.strip()\n",
    "                            rows = rows + '\\n' + row\n",
    "                        profile[ele].append(rows.strip())\n",
    "                    else:\n",
    "                        profile[ele].append('')\n",
    "                sleep(randint(3,10))\n",
    "                df = pd.DataFrame(profile)\n",
    "                with pd.ExcelWriter(\"MnA Details.xlsx\",mode=\"a\",engine=\"openpyxl\",if_sheet_exists=\"replace\") as writer:\n",
    "                    df = df.applymap(lambda x: ILLEGAL_CHARACTERS_RE.sub(r'', x) if isinstance(x, str) else x)\n",
    "                    df.to_excel(writer, sheet_name=\"result\")\n",
    "                print(\"company \" + str(i+1) + ' scraped successfully and saved')\n",
    "                driver.back()\n",
    "            except Exception as e:\n",
    "                print('company ' + str(i+1) + ' not able to load due to ' + str(e))\n",
    "                driver.get(dataframe['link'][len(dataframe)-1]+str(page))\n",
    "        count = count +1\n",
    "    except Exception as e:\n",
    "        print('page ' + str(page) + ' not able to load due to ' + str(e))\n",
    "        driver.get(dataframe['link'][len(dataframe)-1]+str(page))\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d03ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb882bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
